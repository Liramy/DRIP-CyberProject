{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist cleaning the dataset is a must.\n",
    "the dataset is orginized like so:\n",
    "dev-articles -> Articles for testing the results\n",
    "train-articles -> Articles for training the model\n",
    "train-labels-task1 -> Labels of propaganda technique in articles\n",
    "train-labels-task2 -> Labels of propaganda technique with lines in articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it is needed to gather labels from the task1 folder, put them in a dict where the article number is the key and the start and end of the propaganda techniques are values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111111111 265 323 ', '111111111 1795 1935 ', '111111111 149 157 ', '111111111 1069 1091 ', '111111111 1334 1462 ', '111111111 1577 1616 ', '111111111 2023 2086 ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Access the articles and sort them\n",
    "articles = os.listdir(\"datasets/train-articles/\")\n",
    "articles.sort()\n",
    "\n",
    "# Access the labels and sort them\n",
    "propagandaLabels = os.listdir(\"datasets/train-labels-task1-span-identification/\")\n",
    "propagandaLabels.sort()\n",
    "\n",
    "# Create the dictonary for the article number and techniques\n",
    "propLabels = {}\n",
    "\n",
    "for i in range(len(articles)):\n",
    "    article = articles[i]\n",
    "    article = os.path.splitext(article)[0] # Remove the .txt in file name\n",
    "    articles[i] = article\n",
    "    articleNumber = article[7:]\n",
    "    \n",
    "    labelPath = \"datasets/train-labels-task1-span-identification/{articleName}.task1-SI.labels\".format(articleName=article)\n",
    "    with open(labelPath) as f:\n",
    "        labels = f.readlines()\n",
    "        \n",
    "        # replace \\t and \\n in tags with \" \" for easier processing later on\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            label = label.replace(\"\\t\", \" \")\n",
    "            label = label.replace(\"\\n\", \" \")\n",
    "            labels[i] = label\n",
    "        propLabels[article] = labels\n",
    "    f.close()\n",
    "    \n",
    "print(propLabels[articles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what I did to taks 1 will now need to occur to task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Appeal_to_Authority 265 323 ', '  Appeal_to_Authority 1795 1935 ', '  Doubt 149 157 ', '  Repetition 1069 1091 ', '  Appeal_to_fear-prejudice 1334 1462 ', '  Appeal_to_fear-prejudice 1577 1616 ', '  Appeal_to_fear-prejudice 1856 1910 ', '  Appeal_to_fear-prejudice 2023 2086 ']\n"
     ]
    }
   ],
   "source": [
    "propagandaLabelsTechnique = os.listdir(\"datasets/train-labels-task2-technique-classification\") # this is where our tags are located for techique\n",
    "## note: in span-identification, techniques that overlap are not split\n",
    "propagandaLabelsTechnique.sort()\n",
    "propLabelsTechnique = {} # Dictionary containing news article as key, and technique as value\n",
    "\n",
    "for i in range(len(articles)):\n",
    "    article = articles[i]\n",
    "    articleNoExt = os.path.splitext(article)[0] # remove the .txt file extension ([2])\n",
    "    articles[i] = articleNoExt # replace articles[i] with the same name but without the .txt extension\n",
    "    articleNumber = articleNoExt.replace('article', '') # remove 'article' to leave just the number\n",
    "    \n",
    "    labelsPath = \"datasets/train-labels-task2-technique-classification/\"+ articleNoExt + \".task2-TC.labels\"\n",
    "    \n",
    "    with open(labelsPath) as f:\n",
    "        labels = f.readlines()\n",
    "        # replace \\t and \\n in tags with \" \" for easier processing later on\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            label = label.replace(articleNumber, \" \")\n",
    "            label = label.replace(\"\\t\", \" \")\n",
    "            label = label.replace(\"\\n\", \" \")\n",
    "            labels[i] = label \n",
    "            #print(tag)\n",
    "        propLabelsTechnique[articleNoExt] = labels\n",
    "    f.close()\n",
    "    \n",
    "print(propLabelsTechnique[articles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dict created, read all of the sentences that have been annotated as \"propaganda\" from the 'train-articles' folder, and put them in a list which will be named 'propSentencesSpan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propSentencesSpan = []\n",
    "\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    \n",
    "    labels = propLabels[article]\n",
    "    \n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        for label in labels:\n",
    "            label = label.split()\n",
    "            start = int(label[1])\n",
    "            end = int(label[2])\n",
    "            \n",
    "            labeledLine = wholeArticle[start:end]\n",
    "            labeledLine = labeledLine.replace(\"\\n\", \" \")\n",
    "            labeledLine = labeledLine.replace(\"\\t\", \" \")\n",
    "          \n",
    "            propSentencesSpan.append(labeledLine)\n",
    "    f.close()\n",
    "    \n",
    "print(propSentencesSpan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with the keys being the propoganda sentences, and the values being their associated propoganda type. This is to setup the data to be put into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with sentences as keys and technique as value\n",
    "propagandaTechniques = {}\n",
    "propagandaTechniques[\"Sentence\"] = []\n",
    "propagandaTechniques[\"Technique\"] = []\n",
    "\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    \n",
    "    labels = propLabelsTechnique[article]\n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        for label in labels:\n",
    "            label = label.split()\n",
    "            propagandaTechniques[\"Technique\"].append(label[0]) # add technique to dictionary\n",
    "            \n",
    "            start = int(label[1])\n",
    "            end = int(label[2])\n",
    "            taggedLine = wholeArticle[start:end]\n",
    "            taggedLine = taggedLine.replace(\"\\n\", \" \")\n",
    "            taggedLine = taggedLine.replace(\"\\t\", \" \")\n",
    "            \n",
    "            propagandaTechniques[\"Sentence\"].append(taggedLine) # add snippet to dictionary\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the list of propaganda senteces that we've gathered, create another list, 'notPropSentences' which will contain sentences from the articles that have not been annotated as propaganda.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
