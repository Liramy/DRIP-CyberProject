{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist cleaning the dataset is a must.\n",
    "the dataset is orginized like so:\n",
    "dev-articles -> Articles for testing the results\n",
    "train-articles -> Articles for training the model\n",
    "train-labels-task1 -> Labels of propaganda technique in articles\n",
    "train-labels-task2 -> Labels of propaganda technique with lines in articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it is needed to gather labels from the task1 folder, put them in a dict where the article number is the key and the start and end of the propaganda techniques are values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111111111 265 323 ', '111111111 1795 1935 ', '111111111 149 157 ', '111111111 1069 1091 ', '111111111 1334 1462 ', '111111111 1577 1616 ', '111111111 2023 2086 ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "articles = os.listdir(\"datasets/train-articles/\") # this is where our news articles are located\n",
    "propagandaTagsSpan = os.listdir(\"datasets/train-labels-task1-span-identification\") # this is where our tags are located\n",
    "articles.sort()\n",
    "propagandaTagsSpan.sort()\n",
    "propTagsSpan = {} # Dictionary containing the news article number as a key, and propoganda snip as values\n",
    "\n",
    "for epoch in range(len(articles)):\n",
    "    article = articles[epoch]\n",
    "    articleNoExt = os.path.splitext(article)[0] # remove the .txt file extension ([2])\n",
    "    articles[epoch] = articleNoExt # replace newsArticles[i] with the same name but without the .txt extension\n",
    "    articleNo = articleNoExt.replace('article', '') # remove 'article' to leave just the number\n",
    "    \n",
    "    tagPath = \"datasets/train-labels-task1-span-identification/\"+ articleNoExt + \".task1-SI.labels\"\n",
    "    with open(tagPath) as f:\n",
    "        tags = f.readlines()\n",
    "        # replace \\t and \\n in tags with \" \" for easier processing later on\n",
    "        for epoch in range(len(tags)):\n",
    "            tag = tags[epoch]\n",
    "            tag = tag.replace(\"\\t\", \" \")\n",
    "            tag = tag.replace(\"\\n\", \" \")\n",
    "            tags[epoch] = tag \n",
    "        propTagsSpan[articleNoExt] =  tags\n",
    "    f.close()\n",
    "\n",
    "print(propTagsSpan[articles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dict created, read all of the sentences that have been annotated as \"propaganda\" from the 'train-articles' folder, and put them in a list which will be named 'propSentencesSpan'."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 9,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next transmission could be more pronounced or stronger\n"
     ]
    }
   ],
   "source": [
    "propSentencesSpan = []\n",
    "\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    \n",
    "    labels = propTagsSpan[article]\n",
    "    \n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        for label in labels:\n",
    "            label = label.split()\n",
    "            start = int(label[1])\n",
    "            end = int(label[2])\n",
    "            \n",
    "            labeledLine = wholeArticle[start:end]\n",
    "            labeledLine = labeledLine.replace(\"\\n\", \" \")\n",
    "            labeledLine = labeledLine.replace(\"\\t\", \" \")\n",
    "          \n",
    "            propSentencesSpan.append(labeledLine)\n",
    "    f.close()\n",
    "    \n",
    "print(propSentencesSpan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with the keys being the propoganda sentences, and the values being their associated propoganda type. This is to setup the data to be put into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the list of propaganda senteces that we've gathered, create another list, 'notPropSentences' which will contain sentences from the articles that have not been annotated as propaganda.*"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 10,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468\n",
      "5468\n",
      "An outbreak of both bubonic plague, which is spread by infected rats via flea bites, and pneumonic plague, spread person to person, has killed more than 200 people in the Indian Ocean island nation since August.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "notPropSentences = []\n",
    "\n",
    "count = 0\n",
    "maxNum = len(propSentencesSpan) # we want an equal number of propaganda and non-propaganda sentences to create a balanced training set\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        \n",
    "        # Remove SPANNED lines of propoganda from articles to detect non-propoganda lines\n",
    "        currentPropSentences = []\n",
    "        tags = propTagsSpan[article]\n",
    "        for tag in tags:\n",
    "            tag = tag.split()\n",
    "            start = int(tag[1])\n",
    "            end = int(tag[2])\n",
    "            taggedLine = wholeArticle[start:end]\n",
    "            taggedLine = taggedLine.replace(\"\\n\", \" \")\n",
    "            taggedLine = taggedLine.replace(\"\\t\", \" \")\n",
    "            currentPropSentences.append(taggedLine)\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(wholeArticle)\n",
    "        for sentence in sentences:\n",
    "            if(count == maxNum):\n",
    "                break\n",
    "            notProp = True\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            sentence = sentence.replace(\"\\t\", \" \")\n",
    "            for propSentence in currentPropSentences:\n",
    "                if(propSentence in sentence):\n",
    "                    notProp = False\n",
    "                    \n",
    "            if(notProp): \n",
    "                count +=1\n",
    "                notPropSentences.append(sentence)\n",
    "\n",
    "print(len(propSentencesSpan))\n",
    "print(len(notPropSentences))\n",
    "print(notPropSentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a dataset for the propaganda items and sentences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 11,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propaganda</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>The next transmission could be more pronounced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>when (the plague) comes again it starts from m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>appeared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>a very, very different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>He also pointed to the presence of the pneumon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Propaganda                                           Sentence\n",
       "0        Yes  The next transmission could be more pronounced...\n",
       "1        Yes  when (the plague) comes again it starts from m...\n",
       "2        Yes                                           appeared\n",
       "3        Yes                             a very, very different\n",
       "4        Yes  He also pointed to the presence of the pneumon..."
      ]
     },
<<<<<<< HEAD
     "execution_count": 31,
=======
     "execution_count": 11,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# In order to use pandas, we have to create a dict where we will store as values, which we can then convert into a Pandas DataFrame\n",
    "sentencesToCSV = {}\n",
    "sentencesToCSV[\"Propaganda\"] = []\n",
    "sentencesToCSV[\"Sentence\"] = []\n",
    "\n",
    "# A special dict for the Keras Logistic Regression Model:\n",
    "\n",
    "sentencesToCSVKeras = {}\n",
    "sentencesToCSVKeras[\"Propaganda\"] = []\n",
    "sentencesToCSVKeras[\"Sentence\"] = []\n",
    "\n",
    "for sentence in propSentencesSpan: \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"Yes\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(1)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "    \n",
    "\n",
    "for sentence in notPropSentences:  \n",
    "    sentence.replace(\"\\n\", \" \")\n",
    "    sentence.replace(\"\\t\", \" \")\n",
    "    \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"No\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(0)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(sentencesToCSV)\n",
    "dfKeras = pd.DataFrame.from_dict(sentencesToCSVKeras)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the train data to a matrix and numpy array.\n",
    "Also spliting the dataset whilst shuffeling to make the testing more accurate and making the model better."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 12,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams are motivated and working hard.\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(df[\"Sentence\"],\n",
    "                                                                      df[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=df[\"Propaganda\"])\n",
    "\n",
    "train_tags = train_tags.to_numpy()\n",
    "train_sentences = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags = test_tags.to_numpy()\n",
    "test_sentences = test_sentences.to_numpy()\n",
    "\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_tags[1])\n",
    "\n",
    "\n",
    "# Do the same thing for the Keras df\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(dfKeras[\"Sentence\"],\n",
    "                                                                      dfKeras[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=dfKeras[\"Propaganda\"])\n",
    "\n",
    "train_tags_keras = train_tags.to_numpy()\n",
    "train_sentences_keras = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags_keras = test_tags.to_numpy()\n",
    "test_sentences_keras = test_sentences.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the numpy arrays into vectors which will, in turn, be turned into an array"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 39,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9842, 13363)\n",
      "(9842,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_sentences)\n",
    "test_counts = count_vect.transform(test_sentences)\n",
    "print(train_counts.shape)\n",
    "print(train_tags.shape)\n",
    "\n",
    "\n",
    "# Same thing but for Keras\n",
    "\n",
    "count_vect_keras = CountVectorizer()\n",
    "train_counts_keras = count_vect_keras.fit_transform(train_sentences_keras).toarray()\n",
    "test_counts_keras = count_vect_keras.transform(test_sentences_keras).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for calculating precision and recall function for mathimatical needs.\n",
    "IMPORTANT NOTE: the functions are not my code, nor are changed from the original source, those are functions commonly used and very much optimized, thus there is no need to change it."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 14,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras import backend as K\n",
    "\n",
    "# The functions below were taken from [3]\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "\n",
    "keras_lr_1 = Sequential() \n",
    "keras_lr_1.add(Dense(input_dim = 13363, units = 1)) # 13229 is the shape of the df for task 1, 1 is output dimension of the test tag which is 0 or 1 \n",
    "keras_lr_1.add(Activation('relu'))\n",
    "keras_lr_1.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy', recall_m, precision_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logistic reggresioin model, using the commonly used parameters for the model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 15,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# What we will use for LogisticRegression\n",
    "clf_lr = LogisticRegression(solver='lbfgs', multi_class=\"ovr\", max_iter=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the train function for making the gradient better."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 16,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf, X_train, y_train, epochs=10):\n",
    "    scores = []\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch:\" + str(epoch) + \"/\" + str(epochs) + \" -- \" + str(datetime.datetime.now()))\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "        scores.append(score)\n",
    "    print(\"Done training.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for percision and calculating how accurate the model is.\n",
    "Another function for recalling the model to advance it based on results."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 17,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actualTags, predictions, classOfInterest):\n",
    "    actualCounter = 0\n",
    "    predCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == predictions[i]:\n",
    "            predCounter += 1\n",
    "            if classOfInterest == actualTags[i]:\n",
    "                actualCounter += 1\n",
    "    return actualCounter/predCounter\n",
    "\n",
    "def recall(actualTags, predictions, classOfInterest):\n",
    "    actualTagCounter = 0\n",
    "    predictionsCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == actualTags[i]:\n",
    "            actualTagCounter += 1\n",
    "            if classOfInterest == predictions[i]:\n",
    "                predictionsCounter += 1\n",
    "   \n",
    "    return predictionsCounter/actualTagCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 10 epochs of basic training, number of epochs will decide how effective the model will be.\n",
    "The accuracy may stagnate because the model is very much serfuce level and only tokenizing sentences without actual word by word detection."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 18,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Accuracy: 0.5\n",
      "Precision: 0.49613097310066223\n",
      "Recall: 1.0\n"
=======
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "77/77 [==============================] - 2s 10ms/step - loss: 0.7518 - accuracy: 0.5836 - recall_m: 0.2761 - precision_m: 0.5279 - val_loss: 0.6391 - val_accuracy: 0.7157 - val_recall_m: 0.8080 - val_precision_m: 0.6813\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.6405 - accuracy: 0.7255 - recall_m: 0.8170 - precision_m: 0.6957 - val_loss: 0.6311 - val_accuracy: 0.6673 - val_recall_m: 0.9256 - val_precision_m: 0.6077\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6164 - accuracy: 0.7345 - recall_m: 0.8520 - precision_m: 0.6934 - val_loss: 0.6969 - val_accuracy: 0.7377 - val_recall_m: 0.8272 - val_precision_m: 0.7015\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6231 - accuracy: 0.7486 - recall_m: 0.8578 - precision_m: 0.7075 - val_loss: 0.6784 - val_accuracy: 0.7377 - val_recall_m: 0.8285 - val_precision_m: 0.6989\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 1.5787 - accuracy: 0.7050 - recall_m: 0.8742 - precision_m: 0.6699 - val_loss: 6.7286 - val_accuracy: 0.4991 - val_recall_m: 0.9970 - val_precision_m: 0.4975\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 6.2347 - accuracy: 0.5091 - recall_m: 0.9428 - precision_m: 0.5049 - val_loss: 6.2515 - val_accuracy: 0.5000 - val_recall_m: 0.9021 - val_precision_m: 0.4967\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 5.8626 - accuracy: 0.5194 - recall_m: 0.8983 - precision_m: 0.5113 - val_loss: 5.5907 - val_accuracy: 0.5101 - val_recall_m: 0.8307 - val_precision_m: 0.5031\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 3.7303 - accuracy: 0.5362 - recall_m: 0.4623 - precision_m: 0.5819 - val_loss: 2.4565 - val_accuracy: 0.7148 - val_recall_m: 0.5996 - val_precision_m: 0.7696\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 2.6127 - accuracy: 0.6936 - recall_m: 0.5081 - precision_m: 0.8310 - val_loss: 2.4648 - val_accuracy: 0.7139 - val_recall_m: 0.5449 - val_precision_m: 0.8189\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 3.7038 - accuracy: 0.6711 - recall_m: 0.6119 - precision_m: 0.5303 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
      "Accuracy: 0.5\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "keras_lr_1.fit(train_counts_keras, train_tags_keras, epochs= 10, batch_size=128, verbose=3, validation_data=(test_counts, test_tags_keras))\n",
    "\n",
    "loss, accuracy1_keras, recall1_keras, precision1_keras = keras_lr_1.evaluate(test_counts, test_tags_keras, verbose=0)\n",
    "\n",
    "print(\"Accuracy:\", accuracy1_keras)\n",
    "print(\"Precision:\", precision1_keras)\n",
    "print(\"Recall:\", recall1_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": 19,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
<<<<<<< HEAD
      "Epoch:1/10 -- 2024-02-23 11:36:50.178135\n",
      "Epoch:2/10 -- 2024-02-23 11:36:50.696135\n",
      "Epoch:3/10 -- 2024-02-23 11:36:51.164134\n",
      "Epoch:4/10 -- 2024-02-23 11:36:51.663134\n",
      "Epoch:5/10 -- 2024-02-23 11:36:52.122135\n",
      "Epoch:6/10 -- 2024-02-23 11:36:52.581135\n",
      "Epoch:7/10 -- 2024-02-23 11:36:53.399135\n",
      "Epoch:8/10 -- 2024-02-23 11:36:53.875134\n",
      "Epoch:9/10 -- 2024-02-23 11:36:54.329136\n",
      "Epoch:10/10 -- 2024-02-23 11:36:54.851136\n",
=======
      "Epoch:1/10 -- 2024-02-18 23:17:50.582969\n",
      "Epoch:2/10 -- 2024-02-18 23:17:51.273715\n",
      "Epoch:3/10 -- 2024-02-18 23:17:51.965838\n",
      "Epoch:4/10 -- 2024-02-18 23:17:52.982632\n",
      "Epoch:5/10 -- 2024-02-18 23:17:53.640029\n",
      "Epoch:6/10 -- 2024-02-18 23:17:54.300503\n",
      "Epoch:7/10 -- 2024-02-18 23:17:55.210294\n",
      "Epoch:8/10 -- 2024-02-18 23:17:56.074222\n",
      "Epoch:9/10 -- 2024-02-18 23:17:56.892296\n",
      "Epoch:10/10 -- 2024-02-18 23:17:57.657264\n",
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
      "Done training.\n",
      "Accuracy: [0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506]\n"
     ]
    }
   ],
   "source": [
    "clf_lr_score = train_model(clf_lr, train_counts, train_tags, 10)\n",
    "y_pred = clf_lr.predict(test_counts)\n",
    "print(\"Accuracy:\",clf_lr_score)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.680073126142596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf_dt = clf_dt.fit(train_counts,train_tags)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf_dt.predict(test_counts)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of the model in the code (with examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save the model and vectorizer we will use `joblib.dump`"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the logistic reggresion model and uplaod to the folder (Saved in github to avoid rerunning the code many times)"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 20,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PropDetectionModel.mdl']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 44,
=======
     "execution_count": 20,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(count_vect, \"CountVectorizer.vct\")\n",
    "joblib.dump(clf_lr, \"PropDetectionModel.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in order to load, just use `joblib.load`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example import of the model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 21,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
<<<<<<< HEAD
    "model = joblib.load('PropDetectionModel.mdl')\n",
    "count_vector = joblib.load('CountVectorizer.vct')"
=======
    "model = joblib.load(\"PropDetectionModel.clf\")"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Then to use, simply vectorize an array of string and insert into the `model.predict`, \n",
    "an array of 0 and 1 will be returned.\n",
    "0 - no propaganda, 1 - propaganda."
=======
    "Example of usage for a single sentence."
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 44,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "input_strings = [\"example text 1\", \"example text 2\", \"example text 3\"]\n",
    "\n",
    "x_value = count_vector.transform(input_strings)\n",
    "print(model.predict(x_value))"
=======
    "sentence = ['In 2000 the 21st century started']\n",
    "print(model.predict(count_vect_keras.transform(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage for an array of sentences (for example an article)"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": 36,
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[0 0 0]\n"
=======
      "[0 1 0 1]\n"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "sentences = ['In 2000 the 21st century started', 'A mitochondrion is an organelle found in the cells of most eukaryotes, such as animals, plants and fungi', 'Mitochondria have a double membrane structure and use aerobic respiration to generate adenosine triphosphate, which is used throughout the cell as a source of chemical energy.']\n",
    "\n",
    "print(model.predict(count_vector.transform(sentences)))"
=======
    "sentences = ['In 2000 the 21st century started','Even though no one has noticed it', \"Now we live as we live\", \"One might belive otherwise\"]\n",
    "print(model.predict(count_vect_keras.transform(sentences)))"
>>>>>>> 16c093e41245428ba1c37bdaa61fe69373196a86
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
