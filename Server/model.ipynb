{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist cleaning the dataset is a must.\n",
    "the dataset is orginized like so:\n",
    "dev-articles -> Articles for testing the results\n",
    "train-articles -> Articles for training the model\n",
    "train-labels-task1 -> Labels of propaganda technique in articles\n",
    "train-labels-task2 -> Labels of propaganda technique with lines in articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it is needed to gather labels from the task1 folder, put them in a dict where the article number is the key and the start and end of the propaganda techniques are values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111111111 265 323 ', '111111111 1795 1935 ', '111111111 149 157 ', '111111111 1069 1091 ', '111111111 1334 1462 ', '111111111 1577 1616 ', '111111111 2023 2086 ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "articles = os.listdir(\"datasets/train-articles/\") # this is where our news articles are located\n",
    "propagandaTagsSpan = os.listdir(\"datasets/train-labels-task1-span-identification\") # this is where our tags are located\n",
    "articles.sort()\n",
    "propagandaTagsSpan.sort()\n",
    "propTagsSpan = {} # Dictionary containing the news article number as a key, and propoganda snip as values\n",
    "\n",
    "for epoch in range(len(articles)):\n",
    "    article = articles[epoch]\n",
    "    articleNoExt = os.path.splitext(article)[0] # remove the .txt file extension ([2])\n",
    "    articles[epoch] = articleNoExt # replace newsArticles[i] with the same name but without the .txt extension\n",
    "    articleNo = articleNoExt.replace('article', '') # remove 'article' to leave just the number\n",
    "    \n",
    "    tagPath = \"datasets/train-labels-task1-span-identification/\"+ articleNoExt + \".task1-SI.labels\"\n",
    "    with open(tagPath) as f:\n",
    "        tags = f.readlines()\n",
    "        # replace \\t and \\n in tags with \" \" for easier processing later on\n",
    "        for epoch in range(len(tags)):\n",
    "            tag = tags[epoch]\n",
    "            tag = tag.replace(\"\\t\", \" \")\n",
    "            tag = tag.replace(\"\\n\", \" \")\n",
    "            tags[epoch] = tag \n",
    "        propTagsSpan[articleNoExt] =  tags\n",
    "    f.close()\n",
    "\n",
    "print(propTagsSpan[articles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dict created, read all of the sentences that have been annotated as \"propaganda\" from the 'train-articles' folder, and put them in a list which will be named 'propSentencesSpan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next transmission could be more pronounced or stronger\n"
     ]
    }
   ],
   "source": [
    "propSentencesSpan = []\n",
    "\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    \n",
    "    labels = propTagsSpan[article]\n",
    "    \n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        for label in labels:\n",
    "            label = label.split()\n",
    "            start = int(label[1])\n",
    "            end = int(label[2])\n",
    "            \n",
    "            labeledLine = wholeArticle[start:end]\n",
    "            labeledLine = labeledLine.replace(\"\\n\", \" \")\n",
    "            labeledLine = labeledLine.replace(\"\\t\", \" \")\n",
    "          \n",
    "            propSentencesSpan.append(labeledLine)\n",
    "    f.close()\n",
    "    \n",
    "print(propSentencesSpan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with the keys being the propoganda sentences, and the values being their associated propoganda type. This is to setup the data to be put into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the list of propaganda senteces that we've gathered, create another list, 'notPropSentences' which will contain sentences from the articles that have not been annotated as propaganda.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468\n",
      "5468\n",
      "An outbreak of both bubonic plague, which is spread by infected rats via flea bites, and pneumonic plague, spread person to person, has killed more than 200 people in the Indian Ocean island nation since August.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "notPropSentences = []\n",
    "\n",
    "count = 0\n",
    "maxNum = len(propSentencesSpan) # we want an equal number of propaganda and non-propaganda sentences to create a balanced training set\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        \n",
    "        # Remove SPANNED lines of propoganda from articles to detect non-propoganda lines\n",
    "        currentPropSentences = []\n",
    "        tags = propTagsSpan[article]\n",
    "        for tag in tags:\n",
    "            tag = tag.split()\n",
    "            start = int(tag[1])\n",
    "            end = int(tag[2])\n",
    "            taggedLine = wholeArticle[start:end]\n",
    "            taggedLine = taggedLine.replace(\"\\n\", \" \")\n",
    "            taggedLine = taggedLine.replace(\"\\t\", \" \")\n",
    "            currentPropSentences.append(taggedLine)\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(wholeArticle)\n",
    "        for sentence in sentences:\n",
    "            if(count == maxNum):\n",
    "                break\n",
    "            notProp = True\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            sentence = sentence.replace(\"\\t\", \" \")\n",
    "            for propSentence in currentPropSentences:\n",
    "                if(propSentence in sentence):\n",
    "                    notProp = False\n",
    "                    \n",
    "            if(notProp): \n",
    "                count +=1\n",
    "                notPropSentences.append(sentence)\n",
    "\n",
    "print(len(propSentencesSpan))\n",
    "print(len(notPropSentences))\n",
    "print(notPropSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propaganda</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>The next transmission could be more pronounced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>when (the plague) comes again it starts from m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>appeared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>a very, very different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>He also pointed to the presence of the pneumon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Propaganda                                           Sentence\n",
       "0        Yes  The next transmission could be more pronounced...\n",
       "1        Yes  when (the plague) comes again it starts from m...\n",
       "2        Yes                                           appeared\n",
       "3        Yes                             a very, very different\n",
       "4        Yes  He also pointed to the presence of the pneumon..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# In order to use pandas, we have to create a dict where we will store as values, which we can then convert into a Pandas DataFrame\n",
    "sentencesToCSV = {}\n",
    "sentencesToCSV[\"Propaganda\"] = []\n",
    "sentencesToCSV[\"Sentence\"] = []\n",
    "\n",
    "# A special dict for the Keras Logistic Regression Model:\n",
    "\n",
    "sentencesToCSVKeras = {}\n",
    "sentencesToCSVKeras[\"Propaganda\"] = []\n",
    "sentencesToCSVKeras[\"Sentence\"] = []\n",
    "\n",
    "for sentence in propSentencesSpan: \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"Yes\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(1)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "    \n",
    "\n",
    "for sentence in notPropSentences:  \n",
    "    sentence.replace(\"\\n\", \" \")\n",
    "    sentence.replace(\"\\t\", \" \")\n",
    "    \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"No\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(0)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(sentencesToCSV)\n",
    "dfKeras = pd.DataFrame.from_dict(sentencesToCSVKeras)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams are motivated and working hard.\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(df[\"Sentence\"],\n",
    "                                                                      df[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=df[\"Propaganda\"])\n",
    "\n",
    "train_tags = train_tags.to_numpy()\n",
    "train_sentences = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags = test_tags.to_numpy()\n",
    "test_sentences = test_sentences.to_numpy()\n",
    "\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_tags[1])\n",
    "\n",
    "\n",
    "# Do the same thing for the Keras df\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(dfKeras[\"Sentence\"],\n",
    "                                                                      dfKeras[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=dfKeras[\"Propaganda\"])\n",
    "\n",
    "train_tags_keras = train_tags.to_numpy()\n",
    "train_sentences_keras = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags_keras = test_tags.to_numpy()\n",
    "test_sentences_keras = test_sentences.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9842, 13363)\n",
      "(9842,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_sentences)\n",
    "test_counts = count_vect.transform(test_sentences)\n",
    "print(train_counts.shape)\n",
    "print(train_tags.shape)\n",
    "\n",
    "\n",
    "# Same thing but for Keras\n",
    "\n",
    "count_vect_keras = CountVectorizer()\n",
    "train_counts_keras = count_vect_keras.fit_transform(train_sentences_keras).toarray()\n",
    "test_counts_keras = count_vect_keras.transform(test_sentences_keras).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras import backend as K\n",
    "\n",
    "# The functions below were taken from [3]\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "\n",
    "keras_lr_1 = Sequential() \n",
    "keras_lr_1.add(Dense(input_dim = 13363, units = 1)) # 13229 is the shape of the df for task 1, 1 is output dimension of the test tag which is 0 or 1 \n",
    "keras_lr_1.add(Activation('relu'))\n",
    "keras_lr_1.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy', recall_m, precision_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# What we will use for LogisticRegression\n",
    "clf_lr = LogisticRegression(solver='lbfgs', multi_class=\"ovr\", max_iter=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf, X_train, y_train, epochs=10):\n",
    "    scores = []\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch:\" + str(epoch) + \"/\" + str(epochs) + \" -- \" + str(datetime.datetime.now()))\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "        scores.append(score)\n",
    "    print(\"Done training.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actualTags, predictions, classOfInterest):\n",
    "    actualCounter = 0\n",
    "    predCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == predictions[i]:\n",
    "            predCounter += 1\n",
    "            if classOfInterest == actualTags[i]:\n",
    "                actualCounter += 1\n",
    "    return actualCounter/predCounter\n",
    "\n",
    "def recall(actualTags, predictions, classOfInterest):\n",
    "    actualTagCounter = 0\n",
    "    predictionsCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == actualTags[i]:\n",
    "            actualTagCounter += 1\n",
    "            if classOfInterest == predictions[i]:\n",
    "                predictionsCounter += 1\n",
    "   \n",
    "    return predictionsCounter/actualTagCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "77/77 [==============================] - 3s 19ms/step - loss: 0.8541 - accuracy: 0.5409 - recall_m: 0.2168 - precision_m: 0.4364 - val_loss: 0.6810 - val_accuracy: 0.7066 - val_recall_m: 0.5456 - val_precision_m: 0.8062\n",
      "Accuracy: 0.7065813541412354\n",
      "Precision: 0.8127572536468506\n",
      "Recall: 0.5481917262077332\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "keras_lr_1.fit(train_counts_keras, train_tags_keras, epochs= 1, batch_size=128, verbose=1, validation_data=(test_counts, test_tags_keras))\n",
    "\n",
    "loss, accuracy1_keras, recall1_keras, precision1_keras = keras_lr_1.evaluate(test_counts, test_tags_keras, verbose=0)\n",
    "\n",
    "print(\"Accuracy:\", accuracy1_keras)\n",
    "print(\"Precision:\", precision1_keras)\n",
    "print(\"Recall:\", recall1_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch:1/1 -- 2024-02-17 17:51:10.105392\n",
      "Done training.\n",
      "Accuracy: [0.9348709611867506]\n"
     ]
    }
   ],
   "source": [
    "clf_lr_score = train_model(clf_lr, train_counts, train_tags, 1)\n",
    "y_pred = clf_lr.predict(test_counts)\n",
    "print(\"Accuracy:\",clf_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.680073126142596\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Model Accuracy, how often is the classifier correct?\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m,metrics\u001b[38;5;241m.\u001b[39maccuracy_score(test_tags, y_pred))\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recall(test_tags, y_pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mprecision\u001b[1;34m(actualTags, predictions, classOfInterest)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m classOfInterest \u001b[38;5;241m==\u001b[39m actualTags[i]:\n\u001b[0;32m      8\u001b[0m             actualCounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactualCounter\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mpredCounter\u001b[49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf_dt = clf_dt.fit(train_counts,train_tags)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf_dt.predict(test_counts)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"Precision:\", precision(test_tags, y_pred, \"Yes\"))\n",
    "print(\"Recall:\", recall(test_tags, y_pred, \"Yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PropDetectionModel.clf']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf_lr, \"PropDetectionModel.clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'In 2000 the 21st century started'\n",
    "print(clf_lr.predict(count_vect.transform([sentence])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
