{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist cleaning the dataset is a must.\n",
    "the dataset is orginized like so:\n",
    "dev-articles -> Articles for testing the results\n",
    "train-articles -> Articles for training the model\n",
    "train-labels-task1 -> Labels of propaganda technique in articles\n",
    "train-labels-task2 -> Labels of propaganda technique with lines in articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it is needed to gather labels from the task1 folder, put them in a dict where the article number is the key and the start and end of the propaganda techniques are values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111111111 265 323 ', '111111111 1795 1935 ', '111111111 149 157 ', '111111111 1069 1091 ', '111111111 1334 1462 ', '111111111 1577 1616 ', '111111111 2023 2086 ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "articles = os.listdir(\"datasets/train-articles/\") # this is where our news articles are located\n",
    "propagandaTagsSpan = os.listdir(\"datasets/train-labels-task1-span-identification\") # this is where our tags are located\n",
    "articles.sort()\n",
    "propagandaTagsSpan.sort()\n",
    "propTagsSpan = {} # Dictionary containing the news article number as a key, and propoganda snip as values\n",
    "\n",
    "for epoch in range(len(articles)):\n",
    "    article = articles[epoch]\n",
    "    articleNoExt = os.path.splitext(article)[0] # remove the .txt file extension ([2])\n",
    "    articles[epoch] = articleNoExt # replace newsArticles[i] with the same name but without the .txt extension\n",
    "    articleNo = articleNoExt.replace('article', '') # remove 'article' to leave just the number\n",
    "    \n",
    "    tagPath = \"datasets/train-labels-task1-span-identification/\"+ articleNoExt + \".task1-SI.labels\"\n",
    "    with open(tagPath) as f:\n",
    "        tags = f.readlines()\n",
    "        # replace \\t and \\n in tags with \" \" for easier processing later on\n",
    "        for epoch in range(len(tags)):\n",
    "            tag = tags[epoch]\n",
    "            tag = tag.replace(\"\\t\", \" \")\n",
    "            tag = tag.replace(\"\\n\", \" \")\n",
    "            tags[epoch] = tag \n",
    "        propTagsSpan[articleNoExt] =  tags\n",
    "    f.close()\n",
    "\n",
    "print(propTagsSpan[articles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dict created, read all of the sentences that have been annotated as \"propaganda\" from the 'train-articles' folder, and put them in a list which will be named 'propSentencesSpan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next transmission could be more pronounced or stronger\n"
     ]
    }
   ],
   "source": [
    "propSentencesSpan = []\n",
    "\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    \n",
    "    labels = propTagsSpan[article]\n",
    "    \n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        for label in labels:\n",
    "            label = label.split()\n",
    "            start = int(label[1])\n",
    "            end = int(label[2])\n",
    "            \n",
    "            labeledLine = wholeArticle[start:end]\n",
    "            labeledLine = labeledLine.replace(\"\\n\", \" \")\n",
    "            labeledLine = labeledLine.replace(\"\\t\", \" \")\n",
    "          \n",
    "            propSentencesSpan.append(labeledLine)\n",
    "    f.close()\n",
    "    \n",
    "print(propSentencesSpan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with the keys being the propoganda sentences, and the values being their associated propoganda type. This is to setup the data to be put into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the list of propaganda senteces that we've gathered, create another list, 'notPropSentences' which will contain sentences from the articles that have not been annotated as propaganda.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468\n",
      "5468\n",
      "An outbreak of both bubonic plague, which is spread by infected rats via flea bites, and pneumonic plague, spread person to person, has killed more than 200 people in the Indian Ocean island nation since August.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "notPropSentences = []\n",
    "\n",
    "count = 0\n",
    "maxNum = len(propSentencesSpan) # we want an equal number of propaganda and non-propaganda sentences to create a balanced training set\n",
    "for article in articles:\n",
    "    artPath = \"datasets/train-articles/\" + article + \".txt\"\n",
    "    with open(artPath, encoding=\"utf-8\") as f:\n",
    "        wholeArticle = f.read()\n",
    "        \n",
    "        # Remove SPANNED lines of propoganda from articles to detect non-propoganda lines\n",
    "        currentPropSentences = []\n",
    "        tags = propTagsSpan[article]\n",
    "        for tag in tags:\n",
    "            tag = tag.split()\n",
    "            start = int(tag[1])\n",
    "            end = int(tag[2])\n",
    "            taggedLine = wholeArticle[start:end]\n",
    "            taggedLine = taggedLine.replace(\"\\n\", \" \")\n",
    "            taggedLine = taggedLine.replace(\"\\t\", \" \")\n",
    "            currentPropSentences.append(taggedLine)\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(wholeArticle)\n",
    "        for sentence in sentences:\n",
    "            if(count == maxNum):\n",
    "                break\n",
    "            notProp = True\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            sentence = sentence.replace(\"\\t\", \" \")\n",
    "            for propSentence in currentPropSentences:\n",
    "                if(propSentence in sentence):\n",
    "                    notProp = False\n",
    "                    \n",
    "            if(notProp): \n",
    "                count +=1\n",
    "                notPropSentences.append(sentence)\n",
    "\n",
    "print(len(propSentencesSpan))\n",
    "print(len(notPropSentences))\n",
    "print(notPropSentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a dataset for the propaganda items and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propaganda</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>The next transmission could be more pronounced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>when (the plague) comes again it starts from m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>appeared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>a very, very different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>He also pointed to the presence of the pneumon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Propaganda                                           Sentence\n",
       "0        Yes  The next transmission could be more pronounced...\n",
       "1        Yes  when (the plague) comes again it starts from m...\n",
       "2        Yes                                           appeared\n",
       "3        Yes                             a very, very different\n",
       "4        Yes  He also pointed to the presence of the pneumon..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# In order to use pandas, we have to create a dict where we will store as values, which we can then convert into a Pandas DataFrame\n",
    "sentencesToCSV = {}\n",
    "sentencesToCSV[\"Propaganda\"] = []\n",
    "sentencesToCSV[\"Sentence\"] = []\n",
    "\n",
    "# A special dict for the Keras Logistic Regression Model:\n",
    "\n",
    "sentencesToCSVKeras = {}\n",
    "sentencesToCSVKeras[\"Propaganda\"] = []\n",
    "sentencesToCSVKeras[\"Sentence\"] = []\n",
    "\n",
    "for sentence in propSentencesSpan: \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"Yes\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(1)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "    \n",
    "\n",
    "for sentence in notPropSentences:  \n",
    "    sentence.replace(\"\\n\", \" \")\n",
    "    sentence.replace(\"\\t\", \" \")\n",
    "    \n",
    "    sentencesToCSV[\"Propaganda\"].append(\"No\")\n",
    "    sentencesToCSV[\"Sentence\"].append(sentence) \n",
    "    \n",
    "    sentencesToCSVKeras[\"Propaganda\"].append(0)\n",
    "    sentencesToCSVKeras[\"Sentence\"].append(sentence)\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(sentencesToCSV)\n",
    "dfKeras = pd.DataFrame.from_dict(sentencesToCSVKeras)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the train data to a matrix and numpy array.\n",
    "Also spliting the dataset whilst shuffeling to make the testing more accurate and making the model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams are motivated and working hard.\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(df[\"Sentence\"],\n",
    "                                                                      df[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=df[\"Propaganda\"])\n",
    "\n",
    "train_tags = train_tags.to_numpy()\n",
    "train_sentences = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags = test_tags.to_numpy()\n",
    "test_sentences = test_sentences.to_numpy()\n",
    "\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_tags[1])\n",
    "\n",
    "\n",
    "# Do the same thing for the Keras df\n",
    "\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(dfKeras[\"Sentence\"],\n",
    "                                                                      dfKeras[\"Propaganda\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=dfKeras[\"Propaganda\"])\n",
    "\n",
    "train_tags_keras = train_tags.to_numpy()\n",
    "train_sentences_keras = train_sentences.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags_keras = test_tags.to_numpy()\n",
    "test_sentences_keras = test_sentences.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the numpy arrays into vectors which will, in turn, be turned into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9842, 13363)\n",
      "(9842,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_sentences)\n",
    "test_counts = count_vect.transform(test_sentences)\n",
    "print(train_counts.shape)\n",
    "print(train_tags.shape)\n",
    "\n",
    "\n",
    "# Same thing but for Keras\n",
    "\n",
    "count_vect_keras = CountVectorizer()\n",
    "train_counts_keras = count_vect_keras.fit_transform(train_sentences_keras).toarray()\n",
    "test_counts_keras = count_vect_keras.transform(test_sentences_keras).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for calculating precision and recall function for mathimatical needs.\n",
    "IMPORTANT NOTE: the functions are not my code, nor are changed from the original source, those are functions commonly used and very much optimized, thus there is no need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras import backend as K\n",
    "\n",
    "# The functions below were taken from [3]\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "\n",
    "keras_lr_1 = Sequential() \n",
    "keras_lr_1.add(Dense(input_dim = 13363, units = 1)) # 13229 is the shape of the df for task 1, 1 is output dimension of the test tag which is 0 or 1 \n",
    "keras_lr_1.add(Activation('relu'))\n",
    "keras_lr_1.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy', recall_m, precision_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logistic reggresioin model, using the commonly used parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# What we will use for LogisticRegression\n",
    "clf_lr = LogisticRegression(solver='lbfgs', multi_class=\"ovr\", max_iter=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the train function for making the gradient better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf, X_train, y_train, epochs=10):\n",
    "    scores = []\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch:\" + str(epoch) + \"/\" + str(epochs) + \" -- \" + str(datetime.datetime.now()))\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_train, y_train)\n",
    "        scores.append(score)\n",
    "    print(\"Done training.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for percision and calculating how accurate the model is.\n",
    "Another function for recalling the model to advance it based on results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actualTags, predictions, classOfInterest):\n",
    "    actualCounter = 0\n",
    "    predCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == predictions[i]:\n",
    "            predCounter += 1\n",
    "            if classOfInterest == actualTags[i]:\n",
    "                actualCounter += 1\n",
    "    return actualCounter/predCounter\n",
    "\n",
    "def recall(actualTags, predictions, classOfInterest):\n",
    "    actualTagCounter = 0\n",
    "    predictionsCounter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if classOfInterest == actualTags[i]:\n",
    "            actualTagCounter += 1\n",
    "            if classOfInterest == predictions[i]:\n",
    "                predictionsCounter += 1\n",
    "   \n",
    "    return predictionsCounter/actualTagCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 10 epochs of basic training, number of epochs will decide how effective the model will be.\n",
    "The accuracy may stagnate because the model is very much serfuce level and only tokenizing sentences without actual word by word detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "77/77 [==============================] - 2s 10ms/step - loss: 0.7518 - accuracy: 0.5836 - recall_m: 0.2761 - precision_m: 0.5279 - val_loss: 0.6391 - val_accuracy: 0.7157 - val_recall_m: 0.8080 - val_precision_m: 0.6813\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.6405 - accuracy: 0.7255 - recall_m: 0.8170 - precision_m: 0.6957 - val_loss: 0.6311 - val_accuracy: 0.6673 - val_recall_m: 0.9256 - val_precision_m: 0.6077\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6164 - accuracy: 0.7345 - recall_m: 0.8520 - precision_m: 0.6934 - val_loss: 0.6969 - val_accuracy: 0.7377 - val_recall_m: 0.8272 - val_precision_m: 0.7015\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6231 - accuracy: 0.7486 - recall_m: 0.8578 - precision_m: 0.7075 - val_loss: 0.6784 - val_accuracy: 0.7377 - val_recall_m: 0.8285 - val_precision_m: 0.6989\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 1.5787 - accuracy: 0.7050 - recall_m: 0.8742 - precision_m: 0.6699 - val_loss: 6.7286 - val_accuracy: 0.4991 - val_recall_m: 0.9970 - val_precision_m: 0.4975\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 6.2347 - accuracy: 0.5091 - recall_m: 0.9428 - precision_m: 0.5049 - val_loss: 6.2515 - val_accuracy: 0.5000 - val_recall_m: 0.9021 - val_precision_m: 0.4967\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 5.8626 - accuracy: 0.5194 - recall_m: 0.8983 - precision_m: 0.5113 - val_loss: 5.5907 - val_accuracy: 0.5101 - val_recall_m: 0.8307 - val_precision_m: 0.5031\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 3.7303 - accuracy: 0.5362 - recall_m: 0.4623 - precision_m: 0.5819 - val_loss: 2.4565 - val_accuracy: 0.7148 - val_recall_m: 0.5996 - val_precision_m: 0.7696\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 2.6127 - accuracy: 0.6936 - recall_m: 0.5081 - precision_m: 0.8310 - val_loss: 2.4648 - val_accuracy: 0.7139 - val_recall_m: 0.5449 - val_precision_m: 0.8189\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 3.7038 - accuracy: 0.6711 - recall_m: 0.6119 - precision_m: 0.5303 - val_loss: 7.7125 - val_accuracy: 0.5000 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
      "Accuracy: 0.5\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "keras_lr_1.fit(train_counts_keras, train_tags_keras, epochs= 10, batch_size=128, verbose=1, validation_data=(test_counts, test_tags_keras))\n",
    "\n",
    "loss, accuracy1_keras, recall1_keras, precision1_keras = keras_lr_1.evaluate(test_counts, test_tags_keras, verbose=0)\n",
    "\n",
    "print(\"Accuracy:\", accuracy1_keras)\n",
    "print(\"Precision:\", precision1_keras)\n",
    "print(\"Recall:\", recall1_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch:1/10 -- 2024-02-18 23:17:50.582969\n",
      "Epoch:2/10 -- 2024-02-18 23:17:51.273715\n",
      "Epoch:3/10 -- 2024-02-18 23:17:51.965838\n",
      "Epoch:4/10 -- 2024-02-18 23:17:52.982632\n",
      "Epoch:5/10 -- 2024-02-18 23:17:53.640029\n",
      "Epoch:6/10 -- 2024-02-18 23:17:54.300503\n",
      "Epoch:7/10 -- 2024-02-18 23:17:55.210294\n",
      "Epoch:8/10 -- 2024-02-18 23:17:56.074222\n",
      "Epoch:9/10 -- 2024-02-18 23:17:56.892296\n",
      "Epoch:10/10 -- 2024-02-18 23:17:57.657264\n",
      "Done training.\n",
      "Accuracy: [0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506, 0.9348709611867506]\n"
     ]
    }
   ],
   "source": [
    "clf_lr_score = train_model(clf_lr, train_counts, train_tags, 10)\n",
    "y_pred = clf_lr.predict(test_counts)\n",
    "print(\"Accuracy:\",clf_lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the logistic reggresion model and uplaod to the folder (Saved in github to avoid rerunning the code many times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PropDetectionModel.clf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf_lr, \"PropDetectionModel.clf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example import of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load(\"PropDetectionModel.clf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of usage for a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but LogisticRegression is expecting 13363 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m sen_vect\u001b[38;5;241m.\u001b[39mfit_transform(sentence)\n\u001b[0;32m      5\u001b[0m sentence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn 2000 the 21st century started\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 6 features, but LogisticRegression is expecting 13363 features as input."
     ]
    }
   ],
   "source": [
    "sentence = ['In 2000 the 21st century started']\n",
    "print(model.predict(count_vect_keras.transform(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage for an array of sentences (for example an article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['In 2000 the 21st century started','Even though no one has noticed it', \"Now we live as we live\", \"One might belive otherwise\"]\n",
    "print(model.predict(count_vect_keras.transform(sentences)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
